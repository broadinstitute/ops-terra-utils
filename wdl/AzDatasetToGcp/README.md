# WDL Input Overview

This WDL copies files from an azure dataset to a gcp bucket. It first copies the files to the VM from azure and then copies up to the GCP bucket.

## inputs defined by file paths
This WDL is designed to be used with `Run workflow with inputs defined by file paths` option.

## Inputs:
 This workflow is designed to use `Run workflow with inputs defined by file paths` option. You can use `Run workflow(s) with inputs defined by data table` option if you set up data table specifically to use this wdl.

| Input Name                      | Description                                                                                                                                                                                                                                                                                                                                       | Type    | Required | Default |
|---------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------|----------|---------|
| **az_file_tsv**                 | Tsv with all of the azure files to be copied. It should include the headers `az_path`, `dataset_id`, `target_url`, and `bytes`                                                                                                                                                                                                                    | File    | Yes      | N/A     |
| **width**                       | How wide the scatter for copy should be. For example, if you are uploading 1k files and you set this to 4 then it will create 4 smaller tsvs of 250 each and run copies on those files.                                                                                                                                                           | Int     | Yes      | N/A     |
| **log_dir**                     | A gcp directory to put the azure logs if something fails.                                                                                                                                                                                                                                                                                         | String  | Yes      | N/A     |
| **max_gb_per_file**             | The maximum gb a file can be to be included in the copy. A VM is created with the maximum gb + 40, so if a very large file could cost a lot of money and not complete before token az sas token expires.                                                                                                                                          | Int     | Yes      | N/A     |
| **skip_too_large_files**        | Set to true if you want to skip over files bigger then `max_gb_per_file` and continue copying everything else over. If set to `false` the whole job will fail if a file is over that amount.                                                                                                                                                      | Boolean | Yes      | N/A     |
| **check_already_copied**        | `check_already_copied` will check every file before splitting up the main tsv to remove any files that have already been copied over. It checks the name and size of the files at the GCP location. This can a lot of extra compute because it will check every file, so only use this if you think there is large amount of data already copied. | Boolean | Yes      | N/A     |
| **minutes_before_reload_token** | How close the sas token can be to expiring before getting a new one. When the Azure SAS token is created it is set to expire in 60 minutes.                                                                                                                                                                                                       | Int     | No       | 30      |
| **docker**                      | Specifies a custom Docker image to use. Optional.                                                                                                                                                                                                                                                                                                 | String  | No       | N/A     |


## Outputs Table:
This script does not generate any outputs directly. However, logs will be provided to track the progress of the dataset transfer, including details about dataset creation, table confirmation, and data ingestion. You can review the logs in the stderr file for information about the transfer process and the status of the ingestion jobs.
