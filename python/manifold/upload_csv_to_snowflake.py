#!/usr/bin/env python3
"""
Upload CSV to Snowflake

This script reads TSV files generated by redcap_xml_parser.py and uploads them to Snowflake.
It performs the following operations:
1. Reads the schema TSV file to understand the table structure
2. Compares the schema with existing Snowflake tables
3. Creates or updates tables as needed
4. Uploads the data from the table content TSV files to Snowflake

The script supports both append and replace modes for data loading.
"""

import os
import argparse
import logging
import pandas as pd
import snowflake.connector
from snowflake.connector.pandas_tools import write_pandas
from typing import Dict, List, Any, Optional, Tuple, Union, Set
import glob

# Configure logging
logging.basicConfig(
    format="%(levelname)s: %(asctime)s : %(message)s",
    level=logging.INFO
)
logger = logging.getLogger(__name__)


class SnowflakeUploader:
    """
    Class for uploading data to Snowflake.

    This class handles the connection to Snowflake, schema comparison,
    table creation, and data upload.
    """

    def __init__(
        self,
        account: str,
        user: str,
        password: str,
        warehouse: str,
        database: str,
        schema: str,
        role: Optional[str] = None
    ):
        """
        Initialize the SnowflakeUploader with connection parameters.

        Args:
            account: Snowflake account name
            user: Snowflake username
            password: Snowflake password
            warehouse: Snowflake warehouse name
            database: Snowflake database name
            schema: Snowflake schema name
            role: Snowflake role name (optional)
        """
        self.account = account
        self.user = user
        self.password = password
        self.warehouse = warehouse
        self.database = database
        self.schema = schema
        self.role = role
        self.conn = None
        self.cursor = None

    def connect(self) -> None:
        """
        Connect to Snowflake.
        """
        logger.info(f"Connecting to Snowflake account: {self.account}")

        connect_params = {
            'user': self.user,
            'password': self.password,
            'account': self.account,
            'warehouse': self.warehouse,
            'database': self.database,
            'schema': self.schema
        }

        if self.role:
            connect_params['role'] = self.role

        try:
            self.conn = snowflake.connector.connect(**connect_params)
            if self.conn is not None:
                self.cursor = self.conn.cursor()
                logger.info("Connected to Snowflake")
            else:
                logger.error("Failed to connect to Snowflake: connection is None")
        except Exception as e:
            logger.error(f"Failed to connect to Snowflake: {e}")
            self.conn = None
            self.cursor = None

    def disconnect(self) -> None:
        """
        Disconnect from Snowflake.
        """
        if self.cursor:
            self.cursor.close()

        if self.conn:
            self.conn.close()

        logger.info("Disconnected from Snowflake")

    def get_existing_tables(self) -> Set[str]:
        """
        Get the list of existing tables in the Snowflake schema.

        Returns:
            Set of table names
        """
        if self.cursor is None:
            logger.error("Cannot get existing tables: cursor is None")
            return set()

        try:
            self.cursor.execute(f"SHOW TABLES IN {self.database}.{self.schema}")
            result = self.cursor.fetchall()

            # Extract table names from the result
            tables = {row[1].lower() for row in result}

            logger.info(f"Found {len(tables)} existing tables in {self.database}.{self.schema}")

            return tables
        except Exception as e:
            logger.error(f"Failed to get existing tables: {e}")
            return set()

    def get_table_schema(self, table_name: str) -> Dict[str, Dict[str, Any]]:
        """
        Get the schema of an existing table in Snowflake.

        Args:
            table_name: Name of the table

        Returns:
            Dictionary mapping column names to their properties
        """
        if self.cursor is None:
            logger.error(f"Cannot get schema for table {table_name}: cursor is None")
            return {}

        try:
            self.cursor.execute(f"DESCRIBE TABLE {self.database}.{self.schema}.{table_name}")
            result = self.cursor.fetchall()

            # Extract column information from the result
            columns = {}
            for row in result:
                column_name = row[0].lower()
                data_type = row[1]
                nullable = row[3] == 'Y'

                columns[column_name] = {
                    'data_type': data_type,
                    'nullable': nullable
                }

            logger.info(f"Retrieved schema for table {table_name} with {len(columns)} columns")

            return columns
        except Exception as e:
            logger.error(f"Failed to get schema for table {table_name}: {e}")
            return {}

    def compare_schemas(
        self,
        table_name: str,
        new_schema: pd.DataFrame,
        ignore_differences: bool = False
    ) -> Tuple[bool, List[str]]:
        """
        Compare the new schema with the existing schema in Snowflake.

        Args:
            table_name: Name of the table
            new_schema: DataFrame containing the new schema
            ignore_differences: Whether to ignore schema differences

        Returns:
            Tuple of (schemas_match, differences)
        """
        # Filter the schema DataFrame to only include columns for this table
        table_schema = new_schema[new_schema['table_name'] == table_name]

        if table_schema.empty:
            logger.warning(f"No schema information found for table {table_name}")
            return False, ["No schema information found"]

        # Get the existing schema from Snowflake
        existing_schema = self.get_table_schema(table_name)

        # Compare the schemas
        differences = []
        schemas_match = True

        for _, row in table_schema.iterrows():
            column_name = row['column_name'].lower()

            # Check if the column exists in the existing schema
            if column_name not in existing_schema:
                differences.append(f"Column {column_name} does not exist in the existing schema")
                schemas_match = False
                continue

            # Check if the data types match
            snowflake_data_type = row['snowflake_data_type']
            existing_data_type = existing_schema[column_name]['data_type']

            if snowflake_data_type.lower() != existing_data_type.lower():
                differences.append(
                    f"Data type mismatch for column {column_name}: "
                    f"new={snowflake_data_type}, existing={existing_data_type}"
                )
                schemas_match = False

            # Check if the nullability matches
            required = row['required'] == 'Yes'
            nullable = existing_schema[column_name]['nullable']

            if required and nullable:
                differences.append(
                    f"Nullability mismatch for column {column_name}: "
                    f"new=NOT NULL, existing=NULL"
                )
                schemas_match = False

        # Check for columns in the existing schema that are not in the new schema
        for column_name in existing_schema:
            if column_name not in table_schema['column_name'].str.lower().values:
                differences.append(f"Column {column_name} exists in the existing schema but not in the new schema")
                schemas_match = False

        if schemas_match:
            logger.info(f"Schemas match for table {table_name}")
        else:
            if ignore_differences:
                logger.warning(
                    f"Schemas do not match for table {table_name}, but differences are being ignored. "
                    f"Found {len(differences)} differences."
                )
            else:
                logger.error(
                    f"Schemas do not match for table {table_name}. "
                    f"Found {len(differences)} differences."
                )
                for diff in differences:
                    logger.error(f"  - {diff}")

        return schemas_match or ignore_differences, differences

    def create_table(self, table_name: str, schema_df: pd.DataFrame) -> None:
        """
        Create a table in Snowflake based on the schema.

        Args:
            table_name: Name of the table to create
            schema_df: DataFrame containing the schema information
        """
        # Filter the schema DataFrame to only include columns for this table
        table_schema = schema_df[schema_df['table_name'] == table_name]

        if table_schema.empty:
            logger.warning(f"No schema information found for table {table_name}")
            return

        # Build the CREATE TABLE statement
        columns = []
        primary_keys = []

        for _, row in table_schema.iterrows():
            column_name = row['column_name']
            data_type = row['snowflake_data_type']
            required = row['required'] == 'Yes'
            primary_key = row['primary_key']

            column_def = f'"{column_name}" {data_type}'

            if required:
                column_def += ' NOT NULL'

            columns.append(column_def)

            if primary_key:
                primary_keys.append(f'"{column_name}"')

        # Add primary key constraint if there are primary keys
        if primary_keys:
            columns.append(f"PRIMARY KEY ({', '.join(primary_keys)})")

        create_table_sql = f"""
        CREATE TABLE {self.database}.{self.schema}.{table_name} (
            {','.join(columns)}
        )
        """

        logger.info(f"Creating table {table_name}")
        logger.debug(f"SQL: {create_table_sql}")

        if self.cursor is None:
            logger.error(f"Cannot create table {table_name}: cursor is None")
            return

        try:
            self.cursor.execute(create_table_sql)
            logger.info(f"Table {table_name} created successfully")
        except Exception as e:
            logger.error(f"Failed to create table {table_name}: {e}")

    def drop_table(self, table_name: str) -> None:
        """
        Drop a table from Snowflake.

        Args:
            table_name: Name of the table to drop
        """
        logger.info(f"Dropping table {table_name}")

        if self.cursor is None:
            logger.error(f"Cannot drop table {table_name}: cursor is None")
            return

        try:
            self.cursor.execute(f"DROP TABLE IF EXISTS {self.database}.{self.schema}.{table_name}")
            logger.info(f"Table {table_name} dropped successfully")
        except Exception as e:
            logger.error(f"Failed to drop table {table_name}: {e}")

    def upload_data(
        self,
        table_name: str,
        data_df: pd.DataFrame,
        mode: str = 'replace'
    ) -> None:
        """
        Upload data to a Snowflake table.

        Args:
            table_name: Name of the table to upload to
            data_df: DataFrame containing the data to upload
            mode: Upload mode ('append' or 'replace')
        """
        if mode not in ['append', 'replace']:
            logger.error(f"Invalid upload mode: {mode}. Must be 'append' or 'replace'.")
            return

        logger.info(f"Uploading {len(data_df)} rows to table {table_name} in {mode} mode")

        # If mode is replace, truncate the table first
        if mode == 'replace':
            if self.cursor is None:
                logger.error(f"Cannot truncate table {table_name}: cursor is None")
                return

            try:
                self.cursor.execute(f"TRUNCATE TABLE {self.database}.{self.schema}.{table_name}")
                logger.info(f"Table {table_name} truncated")
            except Exception as e:
                logger.error(f"Failed to truncate table {table_name}: {e}")
                return

        # Upload the data
        if self.conn is None:
            logger.error(f"Cannot upload data to table {table_name}: connection is None")
            return

        try:
            success, num_chunks, num_rows, output = write_pandas(
                conn=self.conn,
                df=data_df,
                table_name=table_name,
                database=self.database,
                schema=self.schema,
                quote_identifiers=True
            )

            if success:
                logger.info(f"Data uploaded successfully to {table_name}: {num_rows} rows in {num_chunks} chunks")
            else:
                logger.error(f"Failed to upload data to {table_name}")
                logger.error(f"Output: {output}")
        except Exception as e:
            logger.error(f"Failed to upload data to table {table_name}: {e}")


def main() -> None:
    """
    Main function to run the Snowflake uploader.

    This function reads TSV files generated by redcap_xml_parser.py and uploads them to Snowflake.
    """
    parser = argparse.ArgumentParser(description='Upload TSV files to Snowflake')

    # Input/output arguments
    parser.add_argument('--input-dir', '-i', type=str, required=True,
                        help='Directory containing the TSV files')

    # Snowflake connection arguments
    parser.add_argument('--account', type=str, required=True,
                        help='Snowflake account name')
    parser.add_argument('--user', type=str, required=True,
                        help='Snowflake username')
    parser.add_argument('--password', type=str, required=True,
                        help='Snowflake password')
    parser.add_argument('--warehouse', type=str, required=True,
                        help='Snowflake warehouse name')
    parser.add_argument('--database', type=str, required=True,
                        help='Snowflake database name')
    parser.add_argument('--schema', type=str, required=True,
                        help='Snowflake schema name')
    parser.add_argument('--role', type=str,
                        help='Snowflake role name')

    # Upload options
    parser.add_argument('--mode', type=str, choices=['append', 'replace'], default='replace',
                        help='Upload mode: append or replace (default: replace)')
    parser.add_argument('--ignore-schema-differences', action='store_true',
                        help='Ignore schema differences and proceed with upload')

    args = parser.parse_args()

    # Check if the input directory exists
    if not os.path.isdir(args.input_dir):
        logger.error(f"Input directory does not exist: {args.input_dir}")
        return

    # Check if the schema file exists
    schema_file = os.path.join(args.input_dir, 'schema.tsv')
    if not os.path.isfile(schema_file):
        logger.error(f"Schema file does not exist: {schema_file}")
        return

    # Read the schema file
    logger.info(f"Reading schema file: {schema_file}")
    schema_df = pd.read_csv(schema_file, sep='\t')

    # Get the list of table files
    table_files = glob.glob(os.path.join(args.input_dir, '*.tsv'))
    table_files = [f for f in table_files if os.path.basename(
        f) != 'schema.tsv' and os.path.basename(f) != 'decoding.tsv']

    if not table_files:
        logger.error(f"No table files found in {args.input_dir}")
        return

    logger.info(f"Found {len(table_files)} table files")

    # Create the Snowflake uploader
    uploader = SnowflakeUploader(
        account=args.account,
        user=args.user,
        password=args.password,
        warehouse=args.warehouse,
        database=args.database,
        schema=args.schema,
        role=args.role
    )

    try:
        # Connect to Snowflake
        uploader.connect()

        # Get the list of existing tables
        existing_tables = uploader.get_existing_tables()

        # Process each table file
        for table_file in table_files:
            table_name = os.path.splitext(os.path.basename(table_file))[0]

            logger.info(f"Processing table: {table_name}")

            # Read the table data
            data_df = pd.read_csv(table_file, sep='\t')

            if data_df.empty:
                logger.warning(f"No data found in {table_file}")
                continue

            # Check if the table exists
            if table_name.lower() in existing_tables:
                logger.info(f"Table {table_name} already exists")

                # Compare schemas
                schemas_match, _ = uploader.compare_schemas(
                    table_name=table_name,
                    new_schema=schema_df,
                    ignore_differences=args.ignore_schema_differences
                )

                if not schemas_match:
                    logger.error(f"Schema mismatch for table {table_name}. Skipping upload.")
                    continue

                # Upload data
                uploader.upload_data(
                    table_name=table_name,
                    data_df=data_df,
                    mode=args.mode
                )
            else:
                logger.info(f"Table {table_name} does not exist. Creating it.")

                # Create the table
                uploader.create_table(
                    table_name=table_name,
                    schema_df=schema_df
                )

                # Upload data
                uploader.upload_data(
                    table_name=table_name,
                    data_df=data_df,
                    mode='replace'  # Always use replace mode for new tables
                )

    finally:
        # Disconnect from Snowflake
        uploader.disconnect()


if __name__ == "__main__":
    main()
